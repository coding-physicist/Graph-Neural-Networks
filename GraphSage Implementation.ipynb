{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# GraphSage Implementation\n",
    "\n",
    "This notebook implements and evaluates GraphSage (Graph Sample and Aggregate) for node classification tasks using the Pubmed citation network dataset. GraphSage enables inductive learning on large graphs through neighbor sampling and aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "Import necessary libraries and load the Pubmed citation network dataset for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout\n",
    "\n",
    "# PyTorch Geometric for graph neural networks\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Visualization and utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pubmed citation network dataset\n",
    "dataset = Planetoid(root='.', name='Pubmed')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'PUBMED DATASET STATISTICS':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset Name:      {dataset.name}\")\n",
    "print(f\"Number of Graphs:  {len(dataset):,}\")\n",
    "print(f\"Number of Nodes:   {data.x.shape[0]:,}\")\n",
    "print(f\"Number of Edges:   {data.edge_index.shape[1]:,}\")\n",
    "print(f\"Node Features:     {dataset.num_features}\")\n",
    "print(f\"Number of Classes: {dataset.num_classes}\")\n",
    "print(f\"Train Nodes:       {data.train_mask.sum().item():,}\")\n",
    "print(f\"Validation Nodes:  {data.val_mask.sum().item():,}\")\n",
    "print(f\"Test Nodes:        {data.test_mask.sum().item():,}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neighbor-sampling-section",
   "metadata": {},
   "source": [
    "## Neighbor Sampling\n",
    "\n",
    "GraphSage uses neighbor sampling to create mini-batches for scalable training on large graphs. This enables inductive learning by sampling fixed-size neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neighbor-loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neighbor sampling loaders\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[5, 10],  # Sample 5 neighbors in first layer, 10 in second\n",
    "    batch_size=16,\n",
    "    input_nodes=data.train_mask,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation and test loaders (smaller batches for memory efficiency)\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[5, 10],\n",
    "    batch_size=16,\n",
    "    input_nodes=data.val_mask,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[5, 10],\n",
    "    batch_size=16,\n",
    "    input_nodes=data.test_mask,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neighbor sampling structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"{'NEIGHBOR SAMPLING VISUALIZATION':^60}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print first few subgraphs to understand sampling\n",
    "for i, subgraph in enumerate(train_loader):\n",
    "    if i >= 4:  # Show only first 4 batches\n",
    "        break\n",
    "    print(f\"Subgraph {i}: {subgraph}\")\n",
    "    \n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Each subgraph contains:\")\n",
    "print(\"- x: Node features for sampled neighborhood\")\n",
    "print(\"- edge_index: Edges within the sampled subgraph\")\n",
    "print(\"- y: Labels for all nodes in subgraph\")\n",
    "print(\"- batch_size: Number of target nodes for this batch\")\n",
    "print(\"- input_id: Original node IDs of target nodes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-section",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for model training and evaluation with mini-batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculate classification accuracy\"\"\"\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"Evaluate model on given data loader\"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        pred = out[:batch.batch_size].argmax(dim=1)\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        \n",
    "        total_correct += (pred == y).sum().item()\n",
    "        total_samples += batch.batch_size\n",
    "    \n",
    "    return total_correct / total_samples\n",
    "\n",
    "def plot_training_curves(train_losses, train_accs, val_accs):\n",
    "    \"\"\"Plot training curves for loss and accuracy\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax1.plot(train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training vs Validation Accuracy')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_subgraph_structure(subgraph, title=\"Sampled Subgraph Structure\"):\n",
    "    \"\"\"Visualize the structure of a sampled subgraph\"\"\"\n",
    "    # Convert to NetworkX graph\n",
    "    edge_index = subgraph.edge_index.cpu().numpy()\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges\n",
    "    edges = [(edge_index[0][i], edge_index[1][i]) for i in range(edge_index.shape[1])]\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Color nodes based on whether they are target nodes or neighbors\n",
    "    target_nodes = list(range(subgraph.batch_size))\n",
    "    neighbor_nodes = list(range(subgraph.batch_size, subgraph.x.shape[0]))\n",
    "    \n",
    "    # Draw neighbor nodes (smaller, light blue)\n",
    "    if neighbor_nodes:\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=neighbor_nodes, \n",
    "                             node_color='lightblue', node_size=100, alpha=0.7)\n",
    "    \n",
    "    # Draw target nodes (larger, red)\n",
    "    if target_nodes:\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=target_nodes, \n",
    "                             node_color='red', node_size=300, alpha=0.8)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, width=1)\n",
    "    \n",
    "    # Add labels for target nodes only\n",
    "    target_labels = {i: f'T{i}' for i in target_nodes}\n",
    "    nx.draw_networkx_labels(G, pos, labels=target_labels, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title(f\"{title}\\nRed: Target Nodes ({len(target_nodes)}), Blue: Sampled Neighbors ({len(neighbor_nodes)})\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sampling_statistics(train_loader, num_batches=10):\n",
    "    \"\"\"Plot statistics about the sampling process\"\"\"\n",
    "    batch_sizes = []\n",
    "    subgraph_sizes = []\n",
    "    edge_counts = []\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        batch_sizes.append(batch.batch_size)\n",
    "        subgraph_sizes.append(batch.x.shape[0])\n",
    "        edge_counts.append(batch.edge_index.shape[1])\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Batch sizes\n",
    "    ax1.bar(range(len(batch_sizes)), batch_sizes, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Target Nodes per Batch')\n",
    "    ax1.set_xlabel('Batch Index')\n",
    "    ax1.set_ylabel('Number of Target Nodes')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subgraph sizes\n",
    "    ax2.bar(range(len(subgraph_sizes)), subgraph_sizes, color='lightgreen', alpha=0.7)\n",
    "    ax2.set_title('Total Nodes per Subgraph')\n",
    "    ax2.set_xlabel('Batch Index')\n",
    "    ax2.set_ylabel('Total Nodes (Target + Neighbors)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Edge counts\n",
    "    ax3.bar(range(len(edge_counts)), edge_counts, color='salmon', alpha=0.7)\n",
    "    ax3.set_title('Edges per Subgraph')\n",
    "    ax3.set_xlabel('Batch Index')\n",
    "    ax3.set_ylabel('Number of Edges')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sampling ratio\n",
    "    sampling_ratios = [sg/bs for sg, bs in zip(subgraph_sizes, batch_sizes)]\n",
    "    ax4.bar(range(len(sampling_ratios)), sampling_ratios, color='gold', alpha=0.7)\n",
    "    ax4.set_title('Sampling Expansion Ratio')\n",
    "    ax4.set_xlabel('Batch Index')\n",
    "    ax4.set_ylabel('Total Nodes / Target Nodes')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{'SAMPLING STATISTICS':^50}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Average target nodes per batch: {np.mean(batch_sizes):.1f}\")\n",
    "    print(f\"Average total nodes per subgraph: {np.mean(subgraph_sizes):.1f}\")\n",
    "    print(f\"Average edges per subgraph: {np.mean(edge_counts):.1f}\")\n",
    "    print(f\"Average sampling expansion: {np.mean(sampling_ratios):.1f}x\")\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphsage-section",
   "metadata": {},
   "source": [
    "## GraphSage Implementation\n",
    "\n",
    "Implementation of GraphSage using PyTorch Geometric's SAGEConv layers with neighbor sampling and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphsage-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(torch.nn.Module):\n",
    "    \"\"\"GraphSage model for inductive node classification\"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_h, dim_out, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.convs.append(SAGEConv(dim_in, dim_h))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(dim_h, dim_h))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(SAGEConv(dim_h, dim_out))\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply GraphSage layers\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Final layer (no activation)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, epochs, device='cpu', lr=0.01):\n",
    "        \"\"\"Train the GraphSage model using mini-batch training\"\"\"\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "        # Training history\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"{'Training GraphSage (Mini-batch)':^60}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'Epoch':>5} {'Train Loss':>12} {'Train Acc':>12} {'Val Acc':>12}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for epoch in range(epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                out = self(batch.x, batch.edge_index)\n",
    "                loss = criterion(out[:batch.batch_size], batch.y[:batch.batch_size])\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pred = out[:batch.batch_size].argmax(dim=1)\n",
    "                total_correct += (pred == batch.y[:batch.batch_size]).sum().item()\n",
    "                total_samples += batch.batch_size\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            train_acc = total_correct / total_samples\n",
    "            val_acc = evaluate_model(self, val_loader, device)\n",
    "            \n",
    "            # Store history\n",
    "            train_losses.append(avg_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"{epoch:5d} {avg_loss:12.4f} {train_acc*100:11.2f}% {val_acc*100:11.2f}%\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        return train_losses, train_accs, val_accs\n",
    "        \n",
    "    def test(self, test_loader, device='cpu'):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        return evaluate_model(self, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Initialize, train and evaluate the GraphSage model using mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphsage-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GraphSage model\n",
    "model = GraphSage(dataset.num_features, 256, dataset.num_classes, num_layers=2)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"{'GRAPHSAGE ARCHITECTURE':^40}\")\n",
    "print(\"=\"*40)\n",
    "print(model)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Hidden Dimensions: 256\")\n",
    "print(f\"Number of Layers: 2\")\n",
    "print(f\"Neighbor Sampling: [5, 10]\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphsage-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GraphSage model\n",
    "train_losses, train_accs, val_accs = model.fit(\n",
    "    train_loader, val_loader, epochs=100, device=device, lr=0.01\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_acc = model.test(test_loader, device=device)\n",
    "\n",
    "print(f\"\\n{'='*30}\")\n",
    "print(f\"{'GRAPHSAGE FINAL RESULTS':^30}\")\n",
    "print(f\"{'='*30}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:6.2f}%\")\n",
    "print(f\"{'='*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curves(train_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subgraph-analysis-section",
   "metadata": {},
   "source": [
    "## Subgraph Structure Analysis\n",
    "\n",
    "Analyze and visualize the sampled subgraphs to understand GraphSage's neighbor sampling mechanism in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-subgraphs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot structure of first few subgraphs\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'SUBGRAPH STRUCTURE PLOTS':^50}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a fresh loader to avoid exhausting the iterator\n",
    "viz_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[5, 10],\n",
    "    batch_size=16,\n",
    "    input_nodes=data.train_mask,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "subgraph_list = []\n",
    "for i, subgraph in enumerate(viz_loader):\n",
    "    if i >= 3:  # Plot first 3 subgraphs\n",
    "        break\n",
    "    subgraph_list.append(subgraph)\n",
    "    plot_subgraph_structure(subgraph, f\"Subgraph {i+1}\")\n",
    "    \n",
    "    print(f\"Subgraph {i+1} Details:\")\n",
    "    print(f\"  - Target nodes: {subgraph.batch_size}\")\n",
    "    print(f\"  - Total nodes: {subgraph.x.shape[0]}\")\n",
    "    print(f\"  - Edges: {subgraph.edge_index.shape[1]}\")\n",
    "    print(f\"  - Expansion ratio: {subgraph.x.shape[0]/subgraph.batch_size:.1f}x\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampling-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sampling statistics across multiple batches\n",
    "stats_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[5, 10],\n",
    "    batch_size=16,\n",
    "    input_nodes=data.train_mask,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "plot_sampling_statistics(stats_loader, num_batches=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-insights",
   "metadata": {},
   "source": [
    "### Visualization Insights\n",
    "\n",
    "**Subgraph Structure:**\n",
    "- **Red nodes**: Target nodes for which we want to compute embeddings\n",
    "- **Blue nodes**: Sampled neighbors used for aggregation\n",
    "- **Edges**: Connections within the sampled neighborhood\n",
    "- **Expansion ratio**: How much the neighborhood grows from target nodes\n",
    "\n",
    "**Sampling Benefits:**\n",
    "- **Fixed complexity**: Each target node has bounded neighborhood size\n",
    "- **Scalability**: Memory usage doesn't depend on full graph size\n",
    "- **Parallelization**: Multiple subgraphs can be processed simultaneously\n",
    "- **Inductive capability**: New nodes can be processed without retraining\n",
    "\n",
    "**Key Observations:**\n",
    "- Subgraph sizes vary based on local graph density\n",
    "- Sampling creates diverse neighborhood structures\n",
    "- Edge connectivity patterns reflect original graph topology\n",
    "- Expansion ratios show efficiency of neighbor sampling strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## Technical Analysis\n",
    "\n",
    "Understanding the GraphSage implementation and its advantages for large-scale graph learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphsage-theory",
   "metadata": {},
   "source": [
    "### GraphSage (Graph Sample and Aggregate)\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Inductive Learning**: Can generalize to unseen nodes and graphs during inference\n",
    "- **Neighbor Sampling**: Samples fixed-size neighborhoods to enable mini-batch training\n",
    "- **Aggregation Functions**: Combines neighbor information (mean, max, LSTM, etc.)\n",
    "- **Scalability**: Handles large graphs through sampling and batching\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Memory Efficient**: Fixed computational complexity per node regardless of graph size\n",
    "- **Inductive**: Can handle dynamic graphs and new nodes without retraining\n",
    "- **Parallelizable**: Mini-batch training enables GPU acceleration\n",
    "- **Flexible**: Works with various aggregation functions and sampling strategies\n",
    "\n",
    "**Architecture Details:**\n",
    "- **Two SAGE Layers**: Input → Hidden (256) → Output (3 classes)\n",
    "- **Neighbor Sampling**: [5, 10] neighbors per layer\n",
    "- **Aggregation**: Mean aggregation (default in SAGEConv)\n",
    "- **Regularization**: 50% dropout between layers\n",
    "- **Optimization**: Adam optimizer with weight decay\n",
    "\n",
    "**Sampling Strategy:**\n",
    "- **Layer 1**: Sample 5 neighbors for each target node\n",
    "- **Layer 2**: Sample 10 neighbors for each node from layer 1\n",
    "- **Mini-batches**: Process 16 nodes at a time during training\n",
    "- **Computational Graph**: Each node's representation depends on sampled neighborhood\n",
    "\n",
    "**Expected Performance:**\n",
    "GraphSage typically achieves 75-80% accuracy on Pubmed while being much more scalable than full-batch methods like GCN or GAT. The inductive capability makes it ideal for large, dynamic graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-section",
   "metadata": {},
   "source": [
    "### GraphSage vs Other GNNs\n",
    "\n",
    "**Transductive Methods (GCN, GAT):**\n",
    "- Require the entire graph during training\n",
    "- Cannot handle new nodes without retraining\n",
    "- Memory usage scales with graph size\n",
    "- Better performance on fixed graphs\n",
    "\n",
    "**GraphSage (Inductive):**\n",
    "- Uses neighbor sampling for scalability\n",
    "- Can generalize to unseen nodes/graphs\n",
    "- Fixed memory usage regardless of graph size\n",
    "- Slightly lower accuracy but much more scalable\n",
    "\n",
    "**When to Use GraphSage:**\n",
    "- Large graphs (>100K nodes)\n",
    "- Dynamic graphs with new nodes\n",
    "- Limited computational resources\n",
    "- Industrial applications requiring scalability\n",
    "- Multi-graph learning scenarios\n",
    "\n",
    "**Aggregation Functions:**\n",
    "- **Mean**: Simple average of neighbor features (default)\n",
    "- **Max**: Element-wise maximum of neighbor features\n",
    "- **LSTM**: Sequential processing of neighbors\n",
    "- **Pool**: Max/mean pooling after linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-section",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Pubmed**: 19,717 nodes, 44,338 edges, 500 features, 3 classes\n",
    "- **Domain**: Citation network of biomedical papers\n",
    "- **Task**: Multi-class node classification\n",
    "- **Challenge**: High-dimensional features with sparse connectivity\n",
    "\n",
    "**Model Complexity:**\n",
    "- **Parameters**: ~260K trainable parameters\n",
    "- **Memory**: O(batch_size × neighbors) per forward pass\n",
    "- **Computation**: Linear in sampled neighborhood size\n",
    "- **Scalability**: Can handle graphs with millions of nodes\n",
    "\n",
    "**Training Efficiency:**\n",
    "- **Mini-batch Size**: 16 nodes per batch\n",
    "- **Sampling**: [5, 10] neighbors per layer\n",
    "- **Epochs**: 100 epochs for convergence\n",
    "- **Time**: ~1-2 minutes on CPU, ~30 seconds on GPU\n",
    "\n",
    "**Key Insights:**\n",
    "- GraphSage enables scalable training on large graphs\n",
    "- Neighbor sampling provides computational efficiency\n",
    "- Inductive learning allows generalization to new nodes\n",
    "- Performance scales well with available computational resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}