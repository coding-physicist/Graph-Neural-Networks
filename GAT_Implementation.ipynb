{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Graph Attention Network (GAT) Implementation\n",
    "\n",
    "This notebook implements and evaluates Graph Attention Networks (GATs) for node classification tasks using the Cora citation network dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "Import necessary libraries and load the Cora citation network dataset for node classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout\n",
    "\n",
    "# PyTorch Geometric for graph neural networks\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora citation network dataset\n",
    "dataset = Planetoid(root='.', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'CORA DATASET STATISTICS':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset Name:      {dataset.name}\")\n",
    "print(f\"Number of Graphs:  {len(dataset):,}\")\n",
    "print(f\"Number of Nodes:   {data.x.shape[0]:,}\")\n",
    "print(f\"Number of Edges:   {data.edge_index.shape[1]:,}\")\n",
    "print(f\"Node Features:     {dataset.num_features}\")\n",
    "print(f\"Number of Classes: {dataset.num_classes}\")\n",
    "print(f\"Train Nodes:       {data.train_mask.sum().item():,}\")\n",
    "print(f\"Validation Nodes:  {data.val_mask.sum().item():,}\")\n",
    "print(f\"Test Nodes:        {data.test_mask.sum().item():,}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-section",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accuracy-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculate classification accuracy\"\"\"\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gat-section",
   "metadata": {},
   "source": [
    "## Graph Attention Network (GAT)\n",
    "\n",
    "Implementation of Graph Attention Network using PyTorch Geometric's GATv2Conv layers with multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gat-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    \"\"\"Graph Attention Network for node classification\"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)           # First GAT layer with multi-head attention\n",
    "        self.gat2 = GATv2Conv(dim_h * heads, dim_out, heads=1)      # Second GAT layer (single head for output)\n",
    "        self.dropout = Dropout(0.6)                                # Higher dropout for GAT\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply dropout to input features\n",
    "        h = self.dropout(x)\n",
    "        \n",
    "        # First GAT layer with ELU activation\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        h = self.gat2(h, edge_index)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def fit(self, data, epochs):\n",
    "        \"\"\"Train the GAT model\"\"\"\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.01)\n",
    "        self.train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"{'Training Graph Attention Network':^60}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'Epoch':>5} {'Train Loss':>12} {'Train Acc':>12} {'Val Loss':>12} {'Val Acc':>12}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for epoch in range(epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f\"{epoch:5d} {loss.item():12.4f} {acc.item()*100:11.2f}% {val_loss.item():12.4f} {val_acc.item()*100:11.2f}%\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def test(self, data):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        self.eval()\n",
    "        out = self(data.x, data.edge_index)\n",
    "        acc = accuracy(out[data.test_mask].argmax(dim=1), data.y[data.test_mask])\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Initialize, train and evaluate the GAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gat-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GAT model\n",
    "gat = GAT(dataset.num_features, 32, dataset.num_classes, heads=8)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"{'GAT ARCHITECTURE':^40}\")\n",
    "print(\"=\"*40)\n",
    "print(gat)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in gat.parameters()):,}\")\n",
    "print(f\"Attention Heads (Layer 1): 8\")\n",
    "print(f\"Attention Heads (Layer 2): 1\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gat-fit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT model\n",
    "gat.fit(data, 100)\n",
    "gat_test_acc = gat.test(data)\n",
    "\n",
    "print(f\"\\n{'='*30}\")\n",
    "print(f\"{'GAT FINAL RESULTS':^30}\")\n",
    "print(f\"{'='*30}\")\n",
    "print(f\"Test Accuracy: {gat_test_acc.item()*100:6.2f}%\")\n",
    "print(f\"{'='*30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## Technical Analysis\n",
    "\n",
    "Understanding the Graph Attention Network implementation and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gat-theory",
   "metadata": {},
   "source": [
    "### Graph Attention Networks (GATs)\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Attention Mechanism**: Learns importance weights for neighboring nodes dynamically\n",
    "- **Multi-Head Attention**: Uses multiple attention heads to capture different aspects of relationships\n",
    "- **Self-Attention**: Each node attends to its neighbors based on learned attention coefficients\n",
    "- **Masked Attention**: Only considers direct neighbors (defined by graph structure)\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Dynamic Weighting**: Attention weights adapt based on node features, not just graph structure\n",
    "- **Interpretability**: Attention weights provide insights into which neighbors are important\n",
    "- **Inductive Learning**: Can generalize to unseen graph structures\n",
    "- **Multi-Head Diversity**: Different heads can focus on different types of relationships\n",
    "\n",
    "**Architecture Details:**\n",
    "- **Two GAT Layers**: Input → Hidden (32×8 heads) → Output (7 classes)\n",
    "- **Activation**: ELU (Exponential Linear Unit) for better gradient flow\n",
    "- **Regularization**: 60% dropout (higher than GCN due to attention mechanism)\n",
    "- **Optimization**: Adam optimizer with 0.01 learning rate and weight decay\n",
    "\n",
    "**Attention Mechanism:**\n",
    "- **Layer 1**: 8 attention heads, each producing 32-dimensional embeddings\n",
    "- **Layer 2**: Single attention head for final classification\n",
    "- **Concatenation**: Multi-head outputs are concatenated (8×32 = 256 dimensions)\n",
    "\n",
    "**Expected Performance:**\n",
    "GATs typically achieve 81-83% accuracy on Cora, often outperforming GCNs by learning adaptive attention weights that identify the most relevant neighbors for each node's classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-gat-gcn",
   "metadata": {},
   "source": [
    "### GAT vs GCN Comparison\n",
    "\n",
    "**Graph Convolutional Networks (GCNs):**\n",
    "- Fixed aggregation weights (based on degree normalization)\n",
    "- Spectral approach with localized filters\n",
    "- Simpler architecture, fewer parameters\n",
    "- Good baseline performance\n",
    "\n",
    "**Graph Attention Networks (GATs):**\n",
    "- Learned attention weights (adaptive to node features)\n",
    "- Spatial approach with attention mechanism\n",
    "- More complex architecture, more parameters\n",
    "- Better performance on heterogeneous graphs\n",
    "\n",
    "**When to Use GATs:**\n",
    "- Heterogeneous graphs with diverse node types\n",
    "- When interpretability of neighbor importance is needed\n",
    "- Graphs where not all neighbors are equally important\n",
    "- Sufficient training data to learn attention weights effectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}